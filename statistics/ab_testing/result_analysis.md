# Result Analysis

## 1. Evaluation metric analysis

### 1.1. Statistics computation from samples.

- Calculate the statistics of sample distribution 
  - mean rate: $p^{EG}, p^{CG}$ 
  - the standard deviation:$\mathrm{SD}^{EG}, \mathrm{SD}^{CG}$
  - the pooled standard deviation: $s_{pooled} = \sqrt{\frac{(N^{EG}-1)(\mathrm{SD}^{EG})^2 + (N^{CG}-1)(\mathrm{SD}^{CG})^2}{N^{EG}+N^{CG} - 2}}$
- Calculate the statistics of the mean value:
  - difference of the mean: $\Delta \mu^{obs} = p_s^{EG} - p_s^{CG}$ 
  - the standard error of the $\Delta \mu$: $\mathrm{SE} = \frac{s_{pooled}}{\sqrt{N}}$
  - $\mathrm{CI}_{\Delta \mu} = [\Delta \mu^{obs} - z\cdot \mathrm{SE},\Delta \mu^{obs} + z\cdot \mathrm{SE}]$ 
    - $z=z(\alpha)$, i.e. width of CI is determined by $\alpha$
- Check **practical** significance of the evaluation metric:
  - if $d_{\min}$ is in $\mathrm{CI}_{\Delta \mu}$: not practically significant.
  - if $d_{\min}$ not in $\mathrm{CI}_{\Delta \mu}$: practically significant.
  - Also, you can check statistical significance:
    - if $0$ is in $\mathrm{CI}_{\Delta \mu}$: not statistically significant.
    - if $0$ not in $\mathrm{CI}_{\Delta \mu}$: statistically significant.

### 1.2. Examples

We use from cases from an Udacity course as the example. Link: [Udacity-Ab_test](https://classroom.udacity.com/courses/ud257/lessons/4018018619/concepts/40043987150923) or [Udacity-Ab_test(Youtube)](https://youtu.be/6pGDwrJHitw)

Q: the purple ranges (1) - (6) are different confident interval cases (for $\Delta \mu$), analyze the decision we should take.

<div  align="center"><img src=./result_analysis_asset/confidence_interval_cases.png style = "zoom:15%"></div>

**Explanation**:

1. $ \min(\Delta \mu) > d_{\min} \Rightarrow$ practically significant, 
   - I.e., $p_{EG}>p_{CG}+d_{min}$ must stand.
   - Therefore: the EG version are much better than CG version, should launch the new (EG) version.
2. $\Delta \mu$ may be zero, and $\max(\Delta \mu) < d_{\min}$ 
   - I.e. $p_{EG}=p_{CG}$ may stand, and $p_{EG}<p_{CG}+d_{min}$ must stands
   - Since: $p_{EG}<p_{CG}+d_{min}$ must stand.
   - Therefore: the improvement of EG version must be lower than our expectation (i.e. $d_{\min}$), thus, no need to launch it.
3. $\min(\Delta \mu) >0$, but $\max(\Delta \mu) < d_{\min}$ 
   - I.e. $p_{EG}>p_{CG}$ and $p_{EG}<p_{CG}+d_{min}$ must stand.
   - Since: $p_{EG}<p_{CG}+d_{min}$ must stands
   - Therefore: Although there must be an improvement of EG compared to CG, the improvement of must be lower than our expectation (i.e. $d_{\min}$). Thus, no need to launch it.
4. $\min(\Delta \mu) <-d_{\min}$, and $\max(\Delta \mu) > d_{\min}$ 
   - I.e. Both $p_{EG}<p_{CG} - d_{\min}$ or $p_{EG}>p_{CG}+d_{min}$ may stand.
   - Therefore: the EG may be much worse or may be better than our expectation, we are not sure, so need additional tests.
5. $\min(\Delta \mu) <0$, and $\max(\Delta \mu) > d_{\min}$ 
   - I.e. Both $p_{EG}<p_{CG}$ or $p_{EG}>p_{CG}+d_{min}$ may stand.
   - Therefore: the EG may be slightly worse or may be better than our expectation, we are not sure, so need additional tests.
6. $\min(\Delta \mu) >0$, and $\max(\Delta \mu) > d_{\min}$ 
   - I.e. Both $p_{EG}>p_{CG}$ must stand but $p_{EG}>p_{CG}+d_{min}$ may or may not stand.
   - Therefore: the EG must be better than CG, but we are not sure whether it is better than our expectation, so need additional tests.

## 2. Sanity check

Notes: for sanity check, you only need to define a confidence level $\gamma=1-\alpha$, you only need to verify if the difference are statistically significant. No need to define $\beta, d_{\min}$ for sanity check.

### 2.1. Choose the invariant metric

#### 2.1.1. Design principle

**Invariant metric**: metrics that should not change in the control and experimental group.

1. The metric should have equal value in control and experimental group.
   - E.g. We assign same amount of id to the control and experimental group. Then, the visits, cookies, events generated by these ids should be same. Or the id we assign to the control and experimental group may have different property, like activity.

2. The metric should NOT be affected by the difference (variants) between control and experimental group. 
   - I.e. The metric should come up with the quantities "before" the variant happens, to make sure those quanties in experimental/control group have exactly same property.
     - Variant: the different between the experimental and control group
   - E.g. The whole process have three steps: 
      $\begin{aligned}
      &\mathrm{User_{SeeAds}} (U_0)
      \\
      \xrightarrow{\text{Ads}} & \mathrm{User_{See``Purchase"Button}} (U_1)
      \\
      \xrightarrow{\text{\color{red}``Purchase"Button}} & \mathrm{User_{See``Confirm"Button}} (U_2)
      \\
      \xrightarrow{\text{``Confirm"Button}} & \mathrm{User_{BuyProduct}} (U_3)
      \end{aligned}$
      variant: the color of "purchase" button
      evaluation metric: the "purchase"button-click-through-rate $\frac{U_2}{U_1}$.
      - We can use ads-click-through-rate $\frac{U_1}{U_0}$ as invariant metric, because $U_1$ and $U_0$ will be affect (selected) by our change on "Purchase" Button.
      - But we canNOT use "confirm"button-click-through-rate as invariant metrics, because the $U_2$ for experimental/control group may have different property (like color preference) affected/selected by the purchase button .

#### 2.1.2. Typical invariant metrics

- The number of sample/user assigned to each group
- The page-views/num_transactions generated by the users
- Other conversion rate unaffected by the change.

### 2.2. Check the invariant metric

#### 2.2.1. The metric is a ratio: two sample test

If the metric is a rate, E.g., some conversion rate. Then, similar to the result analysis process, we will:

- Calculate the statistics of sample distribution 
  - mean rate: $p_{iv}^{EG}, p_{iv}^{CG}$ 
  - the standard deviation:$\mathrm{SD}_{iv}^{EG}, \mathrm{SD}_{iv}^{CG}$
  - the pooled standard deviation: $s_{pooled} = \sqrt{ \frac{(N^{EG}-1)(\mathrm{SD}_{iv}^{EG})^2 + (N^{CG}-1)(\mathrm{SD}_{iv}^{CG})^2}{N^{EG}+N^{CG} - 2}}$
- Deduct the confidence interval of mean difference:
  - difference of the mean: $\Delta \mu^{obs} = p_s^{EG} - p_s^{CG}$ 
  - the standard error of the $\Delta \mu$: $\mathrm{SE} = \frac{s_{pooled}}{\sqrt{N}}$
  - $\mathrm{CI}_{\Delta \mu} = [ \Delta \mu^{obs} - z_{iv}\cdot \mathrm{SE},\Delta \mu^{obs} + z_{iv}\cdot \mathrm{SE} ]$
    - $z_{iv}=z_{iv}(\alpha_{iv})$. Note: you can choose a different confidence level (i.e. $1-\alpha_{iv}$) accordingly for invariant metric. No need to be same with the evaluation metric. 
- Check **statistical** significance of the invariant metric:
  - if 0 is in $\mathrm{CI}_{\Delta \mu}$: no statistically significant difference, sanity check passed.
  - if 0 not in $\mathrm{CI}_{\Delta \mu}$: the invariant metrics is statistically significantly different. There maybe problem in our experimental design.

<!-- Assume the sample should have eqaul chance (i.e. 0.5) to be sent to  -->

#### 2.2.2. The metric is a quantity value: one-sample test


If the metric is quantity value, e.g. number of sample in each group. Then we can convert it to ratio and then compare. However, different from  two-sample test (compare means of two distributions), here, we use one-sample test (compare mean of one distribution with a constant.)



E.g.: the metric is the number of samples $N_{EG},N_{CG}$, and we expect to evenly distribution the samples to $CG$ and $EG$. 

In this case, evenly distribution means the probability of get sample from the pool of EG and CG is equal, i.e. the probability of a sample sent to EG/CG should be 0.5

Thus, denote $p_{iv}^{EG} := \frac{N_{EG}}{N_{EG}+N_{CG}}$, the null hypothesis should be $H_0: p_s^{EG}=0.5$ (It is also ok to use  $ p_{iv}^{CG}=0.5$, no difference.)

- Calculate the statistics of the sampling process:
  -  Mean probability of sending sample to EG: $p_{iv}^{EG} := \frac{N_{EG}}{N_{EG}+N_{CG}}$
  -  Standard deviation of the pool population $SD$: $\mathrm{SD}^{EG}_{iv} = \sqrt{p_{iv}^{EG}(1-p_{iv}^{EG})}$
- Deduct the confidence interval of the sampling rate:
  - Standard error of mean probability: $\mathrm{SE}^{EG}_{iv} = \frac{\mathrm{SD}^{EG}_{iv}}{\sqrt{N}}$
  - $\mathrm{CI_{p_{iv}^{EG}}} = [p_{iv}^{EG}-\mathrm{SE}^{EG}_{iv}, p_{iv}^{EG}+\mathrm{SE}^{EG}_{iv}]$
- Check **statistical** significance of the invariant metric:
  - if 0.5 is in $\mathrm{CI_{p_{iv}^{EG}}}$: no statistically significant difference, sanity check passed.
  - if 0.5 not in $\mathrm{CI_{p_{iv}^{EG}}}$: the invariant metrics is statistically significantly different. There maybe problem in our experimental design.


**Why one-sample not two-sample? TBD**, personal understanding: Because the sample will either go to EG or CG. So, the $p_{iv}^{EG}+p_{iv}^{CG} = 1$, they are correlated. I.e. they are 1 experiment, an experiment about whether to sending a sample from pool to EG, not two independent experiments, so don't use two sample test. The probability we are testing is the probability of sending a sample from pool to EG.



<!-- 
- Thus, we can set the conversion rate to be the chance the sampling probability of two group from the pool. 
  - I.e.: $p_s^{EG} = \frac{N_{EG}}{N_{EG}+N_{CG}}, p_s^{CG} = \frac{N_{CG}}{N_{EG}+N_{CG}}$ -->
<!-- - Then, follow the "ratio" steps to check if $p_s^{EG}$ and $p_s^{CG}$ are statistically significant.
  - Also, we can check if $p_s^{EG}$ (or $p_s^{CG}) is statistically significantly different from expected sampling rate 0.5, depending on your preference. -->

<!-- - Then, we can check if $p_s^{EG}$ (or $p_s^{CG}$) is statistically significantly different from expected sampling rate 0.5, depending on your preference. (one sample test.)  -->


<!-- (**Remaining Q: can we used two sample test here? between $p_s^{EG}$ and $p_s^{CG}$?**) -->
Ref: [Udacity-Abtesting-Lesson5-Lecture6](https://classroom.udacity.com/courses/ud257/lessons/4085798776/concepts/40713087720923)

#### 2.2.3. Other tricks and notes

**Slicing:** besides gather the whole data and check at once. You can also gather the daily data and do the sanity check day by day, to examine potential problem.

Ref: [Udacity-Abtesting-Lesson5-Lecture6](https://classroom.udacity.com/courses/ud257/lessons/4085798776/concepts/40713087720923)

### 3. possible problems in experiment design (TBD)

- Infrastructure (engineering/technically.)
- Unrealized user filter. I.e. we only include certain kinds of users, not representative to the whole group.
- Data capture problem 1: we are not fetch the data at the correct time or the user behavior changes.
  - Retrospective analysis: check if the metric is invariant in historical data.
  - Slicing: to see if the invariant metric are invariant in certain period or not. If there is some periodical/time effect?
- Data capture problem 2: we are not fetch the data on the right group
  - Cohort analysis: to see if there are cohorts/sub-population in our users.
- Learning effect: the user are adapting the changes. The metric behavior may be different between the earlier stage and later stage.

Ref: [Udacity-Abtesting-Lesson5-Lecture7](https://classroom.udacity.com/courses/ud257/lessons/4085798776/concepts/41174386370923)


## Deep dive: sign test

Ref: [Wiki-Sign test](https://en.wikipedia.org/wiki/Sign_test)

The purpose is to examine the generality (consistence) of our conclusion/assumption. Whether the conclusion we found is a general pattern or affected by some outliers. Or whether the result is affected by some outliers.

"Sign" refers to the sign of difference between experimental group and control group.

- If we want to prove A is larger than B, then, we expect to see the daily $A-B$ have more positive signs. I.e. $H_0: p(+)=0.5$ is rejected ( $p(+)>0.5$ is significant ). 
- If we use it on quantities have equal value, we expect the $+$ and $-$ should be similar. I.e. $H_0: p(+)=0.5$ is accepted ( $p(+)>0.5$  is insignificant ).
- This is also a one-sample test, see sec 2.2.2.

Notes:

- Sign test is a binomial distribution, where each individual time of test give binary result, like the coin flips case.
  - Ab testing is also binomial distribution, each time of test give binary  result.
- Null hypothesis is usually "number of positive sign is equal to number of negative sign."
  - Typically use two-tail p-value, since we don't want either "$N(+)>>N(-)$" or "$N(+)<<N(-)$"

<!-- whether the data of experimental/control group have similar fluctuation. -->

### Example 1. Sanity check

We usually have sign test in the sanity check, to check whether there is a difference on daily sample amount. 

Apparently, the following daily sampling behavior have some problem. And with sign test, we can diagnose the abnormal behavior.

| Group     | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | 
| :-------: | :---: | :---: | :---: | :---: | :---: |
| **Exp**   | 0     | 0     | 0     | 0     | 10    |
|**Control**| 2     | 2     | 2     | 2     | 2     |
| Sign      | -     | -     | -     | -     | +     |

The below example is a normal behavior, i.e., supporting the conclusion "sample size is equal" is consistent/no-problem. (Necessary but not sufficient condition?)

| Group     | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | 
| :-------: | :---: | :---: | :---: | :---: | :---: |
| **Exp**   | 1     | 2     | 1     | 2     | 1     |
|**Control**| 2     | 1     | 2     | 1     | 2     |
| Sign      | -     | +     | -     | +     | -     |

#### Example 2. Evaluation metric check

Let's say, our found $p^{EG}>p^{CG}$ is significant.

But if the sign test is like the following 2 table, then, our conclusion is problematic. (I.e. the difference is not really significant, it is caused by outliers)

| Group     | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | 
| :-------: | :---: | :---: | :---: | :---: | :---: |
| **Exp**   | 0.4   | 0.3   | 0.4   | 0.3   | 1.0   |
|**Control**| 0.3   | 0.4   | 0.3   | 0.4   | 0.4   |
| Sign      | +     | -     | +     | -     | +     |

or even

| Group     | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | 
| :-------: | :---: | :---: | :---: | :---: | :---: |
| **Exp**   | 0.3   | 0.3   | 0.3   | 0.3   | 1.0   |
|**Control**| 0.4   | 0.4   | 0.4   | 0.4   | 0.4   |
| Sign      | -     | -     | -     | -     | +     |

The pattern supporting (necessary to) our conclusion should be like the following. (I.e. p(+) > p(-) is significant. 'practically' or 'statistically' is depednent on your needs.)

| Group     | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | 
| :-------: | :---: | :---: | :---: | :---: | :---: |
| **Exp**   | 0.5   | 0.5   | 0.5   | 0.45  | 0.6   |
|**Control**| 0.4   | 0.4   | 0.4   | 0.4   | 0.4   |
| Sign      | +     | +     | +     | +     | +     |


**personal understanding:** sign test seems to be "necessary but not sufficient" condition for our formal test on evaluation/invariant metrics.

## Deep dive: practically and statistically significant

### Meaning of "significant"

"Practically" or "statistically significant" is in terms of the statement "$p_{EG}>p_{CG}$"

"**Statistically significant**" means "$p_{EG}>p_{CG}$" is statistically/mathematically correct. 

However, if $p_{EG}$ just slightly larger than $p_{CG}$, then, even "$p_{EG}>p_{CG}$" is mathematically correct, there is not too much difference between the experimental version and control version. Then,the experimental version is not worthy to launch. I.e the conclusion "$p_{EG}>p_{CG}$" have no practical use.

Then, how to determine if the conclusion "$p_{EG}>p_{CG}$" have practical use?

Remember that, we set a $d_{\min}$ value. This implies we hope the $p_{EG} \geq p_{CG}+d_{\min}$, then, the experimental version is worthy to launch. (That is why $d_{\min}$ is called "minimum detectable effect".)

Thus, if we can confirm $p_{EG} \geq p_{CG}+d_{\min}$, then, we will say the conclusion "$p_{EG}>p_{CG}$" from the ab-testing is **practically significant**. I.e. the conclusion "$p_{EG}>p_{CG}$" from the ab-testing is of-practically-use/practically-useful. (That is why $d_{\min}$ is also called "practical significance level".)

### Experiment result and "significant"

In our experiment, $\Delta \mu$ is the difference between $p_{EG}$ and $p_{CG}$. I.e. $\Delta \mu = p_{EG} - p_{CG}$

From the above section, we can know:

- If $\Delta \mu>0$ stands, we can claim the result is statistically significant.
- IF $\Delta \mu>d_{min}$ stands, we can claim the result is practically significant.

However, with experiment, we can never get the ground true $\Delta \mu$, $\Delta \mu$ has a probability distribution, mathematically, $\Delta \mu$ could be all value. The only matter is that the probability (density) for different value is different.

Thus, we solve the problem in such way:

<div  align="center"><img src=./result_analysis_asset/statistically_and_practically_significant_1.jpeg style = "zoom:25%"></div>

 By setting the confidence level, we can determine limit the possible value of $\Delta \mu$ to a limit range, i.e. *confidence interval*. And if the minimum possible $\Delta \mu$ value, (i.e. $\Delta \mu_{\min}$), is larger than 0 (i.e. $\Delta \mu_{\min} > 0$), it is reasonable to claim $\Delta \mu$ must be larger than 0 ( i.e. $\Delta \mu >0$ stands). Then, we can claim the result is "statistically significant". I.e.:

$$\begin{aligned}
``\Delta\mu_{\min} >0" & = {\text{``the minimum possible } \Delta \mu \text{ value is larger than 0''}} 
\\ &= `` \Delta \mu \text{ must be larger than 0 "  } 
\\ &= `` p_{EG} \text{ must be larger than }p_{CG}" \rightarrow {``H_0 \text{ is incorrect"}}
\\ &\underset{ \text{i.e.} }{\rightarrow} \text{the result is statistically significant}
\end{aligned}$$

Similarly, when $\Delta \mu > d_{\min}$, we can claim $p_{EG} > p_{CG} + d_{\min}"$ must be correct, then we can claim the result is practically significant.

$$\begin{aligned}
``\Delta\mu_{\min} >d_{\min}" & = {\text{``the minimum possible } \Delta \mu \text{ value is larger than } d_{\min}"} 
\\ &= `` \Delta \mu \text{ must be larger than } d_{\min}"
\\ &= `` p_{EG} \text{ must be larger than }p_{CG} + d_{\min}" \rightarrow {``H_0 \text{ is incorrect"}}
\\&\underset{ \text{i.e.} }{\rightarrow} \text{the result is practically significant}
\end{aligned}$$

Note, the formula of $\Delta \mu_{\min}$:

$$\begin{aligned}
    \Delta \mu_{\min} &:= \min( \mathrm{CI}_{\Delta \mu} )\\ 
    &= \Delta\mu^{obs} - z\cdot \mathrm{SE} \\
    &= (p^{obs}_{EG} - p^{obs}_{CG}) - z\cdot \frac{s_{\mathrm{pooled}}}{\sqrt{N}} 
\end{aligned}$$
where 
- z is determined by confidence level $\gamma$, i.e. $z = z(\gamma)$.
- Confidence level $\gamma$ is determined by significance level $\alpha$, i.e. $\gamma = 1-\alpha$
  - Ref:  [Wiki-Statistical_significance](https://en.wikipedia.org/wiki/Statistical_significance), [Wiki-Confidence_level](https://en.wikipedia.org/wiki/Confidence_interval), [DataScienceCentral-Blog](https://www.datasciencecentral.com/profiles/blogs/significance-level-vs-confidence-level-vs-confidence-interval), [Blog](https://statisticsbyjim.com/hypothesis-testing/hypothesis-tests-confidence-intervals-levels/)


Note: The above discussion is from the point of view of "the location of ground true $\Delta \mu$". So, an alternative understanding is from the point of view of "the location of  the observed $\Delta \mu$ (i.e. $\Delta \mu^{obs}$)"
(The two views are equivalent, since $\Delta \mu_{\min} > 0 \Leftrightarrow \Delta\mu^{obs} - z\cdot\mathrm{SE}>0 \Leftrightarrow \Delta\mu^{obs} > z \cdot SE$. )

<div  align="center"><img src=./result_analysis_asset/statistically_and_practically_significant_2.jpeg style = "zoom:25%"></div>

<!-- 
When $\Delta \mu > 0$, we can infer that  ${``H_0 (p_{EG} = p_{CG} )\text{ is incorrect"}} $ or ${`` p_{EG} \text{ must be larger than }p_{CG}" }$, then we will say the statement "$p_{EG}>p_{CG}$" is statistically significant.



We made decision by the question whether 

$\Delta \mu_{\min}$ is understand as the smallest possible value for $\Delta \mu$, given the experiment, at certain confidence $\mathrm{C}$.  [Note: $z=z(\mathrm{C})$]

$\Delta \mu = p_{EG}-p_{CG}$ -->






<!-- This is because: -->



<!-- 
In our experiment, 

When $\Delta \mu > d_{\min}$: -->


<!-- The judgement "H_0 is wrong" is statistical significant when  -->

<!-- <div  align="center"><img src=./result_analysis_asset/statistically_and_practically_significant.jpeg style = "zoom:25%"></div> -->

Ref: [Udacity-Ab_test](https://classroom.udacity.com/courses/ud257/lessons/4018018619/concepts/40043987150923): practical siginificant and statistical significant

## Deep Dive: Confidence Interval

Confidence interval( $\mathrm{CI}$ ): the range where the ground true value will fall in with the probability of confidence level.

Confidence level ( $\mathrm{C}$ ): the probability that the ground true value fall in confidence interval.

E.g. $\mathrm{CI}$ for mean value in ab testing (z-test.)

<div  align="center"><img src=https://qph.fs.quoracdn.net/main-qimg-87329ae87bcf6e926acfec80f426aa6b.webp style = "zoom:60%"></div>

$$\mathrm{CI := \mu \pm z\cdot \mathrm{SE}} = \mu \pm z \cdot\frac{SD}{\sqrt{N}} $$

where:

- $z$ score: the width of the $\mathrm{CI}$. 
  - A numerical value, the unit is the standard deviation of the distribution. 
  - Determined by confidence level $\mathrm{C}$. E.g. $z(\mathrm{C} = 95\% ) =1.96$
- Note: The distribution here is the distribution of $\Delta \mu$ i.e. $\sim$ population mean. Thus, the standard deviation of the distribution is SE.


Ref: [StatisticsHowTo](https://www.statisticshowto.com/probability-and-statistics/confidence-interval/), [Quora](https://www.quora.com/What-is-the-difference-between-confidence-interval-and-confidence-level), [Wiki-Confidence_interval](https://en.wikipedia.org/wiki/Confidence_interval)